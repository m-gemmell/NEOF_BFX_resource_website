{
  "hash": "1c38440b865455ec310a55cacc636082",
  "result": {
    "markdown": "---\ntitle: \"Algorithmic bias\"\nsubtitle: \"<br><br> Data Science in a Box\"\nauthor: \"[datasciencebox.org](https://datasciencebox.org/)\"\noutput:\n  xaringan::moon_reader:\n    css: [\"../xaringan-themer.css\", \"../slides.css\"]\n    lib_dir: libs\n    anchor_sections: FALSE\n    nature:\n      ratio: \"16:9\"\n      highlightLines: true\n      highlightStyle: solarized-light\n      countIncrementalSlides: false\n---\n\n\n\n\nlayout: true\n  \n<div class=\"my-footer\">\n<span>\n<a href=\"https://datasciencebox.org\" target=\"_blank\">datasciencebox.org</a>\n</span>\n</div> \n\n---\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nclass: middle\n\n# First a bit of fun...\n# The Hathaway Effect\n\n\n---\n\n.pull-left[\n- Oct. 3, 2008: Rachel Getting Married opens, BRK.A up 0.44%  \n- Jan. 5, 2009: Bride Wars opens, BRK.A up 2.61%  \n- Feb. 8, 2010: Valentine’s Day opens, BRK.A up 1.01%  \n- March 5, 2010: Alice in Wonderland opens,  BRK.A up 0.74%  \n- Nov. 24, 2010: Love and Other Drugs opens, BRK.A up 1.62%  \n- Nov. 29, 2010: Anne announced as co-host of the Oscars, BRK.A up 0.25%\n]\n.pull-right[\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](img/hathaway-effect-poster.jpg){fig-align='left' width=60%}\n:::\n:::\n]\n\n.footnote[\n.midi[\nDan Mirvish. [The Hathaway Effect: How Anne Gives Warren Buffett a Rise](https://www.huffpost.com/entry/the-hathaway-effect-how-a_b_830041).  \nThe Huffington Post. 2 Mar 2011.\n]\n]\n\n---\n\n\nclass: middle\n\n# Algorithmic bias and gender\n\n\n---\n\n## Google Translate\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/google-translate-gender-bias.png){fig-align='center' width=100%}\n:::\n:::\n\n---\n\n\n## Amazon's experimental hiring algorithm\n\n- Used AI to give job candidates scores ranging from one to five stars -- much like shoppers rate products on Amazon\n- Amazon's system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way; it taught itself that male candidates were preferable\n\n.pull-left-wide[\n>Gender bias was not the only issue. Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said.\n]\n\n.footnote[\nJeffrey Dastin. [Amazon scraps secret AI recruiting tool that showed bias against women](https://reut.rs/2Od9fPr).  \nReuters. 10 Oct 2018.\n]\n\n\n---\n\nclass: middle\n\n# Algorithmic bias and race\n\n---\n\n\n## Facial recognition\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/guardian-facial-recognition.png){fig-align='center' width=35%}\n:::\n:::\n\n\n.footnote[\n.midi[\nIan Tucker. ['A white mask worked better': why algorithms are not colour blind](https://www.theguardian.com/technology/2017/may/28/joy-buolamwini-when-algorithms-are-racist-facial-recognition-bias).  \nThe Guardian. 28 May 2017.\n]\n]\n\n\n---\n\n## Criminal Sentencing\n\n.center[\nThere’s software used across the country to predict future criminals.  \nAnd it’s biased against blacks.\n]\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/propublica-criminal-sentencing.png){fig-align='center' width=60%}\n:::\n:::\n\n.footnote[\n.midi[\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). 23 May 2016. ProPublica.\n]\n]\n\n---\n\n\n## A tale of two convicts\n\n.pull-left[\n![](img/propublica-prater-broden-1.png)\n]\n--\n.pull-right[\n![](img/propublica-prater-broden-2.png)\n]\n\n\n---\n\nclass: middle\n\n>“Although these measures were crafted with the best of intentions, I am concerned that they inadvertently undermine our efforts to ensure individualized and equal justice,” he said, adding, “they may exacerbate unwarranted and unjust disparities that are already far too common in our criminal justice system and in our society.”\n>  \n>Then U.S. Attorney General Eric Holder (2014)\n\n---\n\n\n## ProPublica analysis\n\n### **Data:**\n\nRisk scores assigned to more than 7,000 people arrested in Broward County, Florida, in 2013 and 2014 + whether they were charged with new crimes over the next two years\n\n\n---\n\n## ProPublica analysis\n\n### **Results:**\n\n- 20% of those predicted to commit violent crimes actually did\n- Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors)\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/propublica-results.png){fig-align='center' width=85%}\n:::\n:::\n- Algorithm was more likely to falsely flag black defendants as future criminals, at almost twice the rate as white defendants\n- White defendants were mislabeled as low risk more often than black defendants\n\n---\n\n\n## How to write a racist AI without trying\n\n.center[\n<iframe width=\"900\" height=\"450\" src=\"https://notstatschat.rbind.io/2018/09/27/how-to-write-a-racist-ai-in-r-without-really-trying/\" frameborder=\"0\" style=\"background:white;\"></iframe>  \n]\n\n.footnote[\n.midi[\nThomas Lumley. [How to write a racist AI in R without really trying](https://notstatschat.rbind.io/2018/09/27/how-to-write-a-racist-ai-in-r-without-really-trying/).  \nBiased and Inefficient. 27 September 2018.\n]\n]\n\n\n---\n\nclass: middle\n\n# Further reading\n\n---\n\n\n## Machine Bias\n\n.pull-left[\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/propublica-machine-bias.png){fig-align='center' width=100%}\n:::\n:::\n\n]\n.pull-right[\n[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  \n<br>\nby Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner\n]\n\n\n---\n\n## Ethics and Data Science\n\n.pull-left[\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/ethics-data-science.jpg){fig-align='center' width=65%}\n:::\n:::\n]\n.pull-right[\n[Ethics and Data Science](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)  \n<br>\nby Mike Loukides, Hilary Mason, DJ Patil  \n(Free Kindle download)\n]\n\n---\n\n\n## Weapons of Math Destruction\n\n.pull-left[\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/weapons-of-math-destruction.jpg){fig-align='center' width=65%}\n:::\n:::\n\n]\n.pull-right[\n[Weapons of Math Destruction](https://www.penguin.co.uk/books/304/304513/weapons-of-math-destruction/9780141985411.html)  \nHow Big Data Increases Inequality and Threatens Democracy  \n<br>\nby Cathy O'Neil\n]\n\n\n---\n\n## Algorithms of Oppression\n\n.pull-left[\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/algorithms-of-oppression.jpg){fig-align='center' width=65%}\n:::\n:::\n]\n.pull-right[\n[Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/)  \nHow Search Engines Reinforce Racism  \n<br>\nby Safiya Umoja Noble\n]\n\n---\n\n\n## Parting thoughts\n\n- At some point during your data science learning journey you will learn tools that can be used unethically\n- You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)\n\n.question[\nHow do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?\n]\n\n\n---\n\n## Do good with data\n\n- Data Science for Social Good: \n  - [The Alan Turing Institute](https://www.turing.ac.uk/collaborate-turing/data-science-social-good)\n  - [University of Chicago](https://dssg.uchicago.edu/)\n- [DataKind](https://www.datakind.org/): DataKind brings high-impact organizations together with leading data scientists to use data science in the service of humanity.\n- Sign the Manifesto for Data Practices: [datapractices.org/manifesto](https://datapractices.org/manifesto/)\n\n---\n\n\n## Further watching\n\n.center[\n<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/fgf2VjnhpCs?start=1162\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>  \n]\n\n.footnote[\n.midi[\nJulien Cornebise. AI for Good in the R and Python ecosystems. useR 2019.\n]\n]\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/font-awesome/css/all.css\" rel=\"stylesheet\" />\n<link href=\"../../../site_libs/font-awesome/css/v4-shims.css\" rel=\"stylesheet\" />\n<link href=\"../../../site_libs/panelset/panelset.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/panelset/panelset.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}